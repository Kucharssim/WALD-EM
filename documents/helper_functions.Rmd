---
title: "Helper functions"
author: "Simon Kucharsky"
date: "`r Sys.Date()`"
output: 
  rmarkdown::github_document:
    pandoc_args: --webtex
bibliography: "`r here::here('bibliography.bib')`"
csl: "`r here::here('apa.csl')`"
---

# Common features of the Stan files

All of the models we implemented share some common common computations, some of which needed to be implemented by specifying custom functions in Stan. These functions are loaded in each of the model's file by including the `load_functions.stan` file in the function block. Specifically, placing `#include stan/helpers/load_functions.stan` inside of `functions{}` block in a `.stan` file makes accessible functions defined in the following files:


```{r echo=FALSE}
writeLines(readLines(here::here("stan", "helpers", "load_functions.stan")))
```

Here, we go through all of the files to document the functionality we added to Stan for fitting the models.

NB. These functions can be exposed to R by the script `source(here::here("R", "expose_helpers_stan.R"))`.

## Wald distribution

We implemented the Wald distribution in Stan. Specifically, we implemented the Wald log likelihood function, and the function that generates data that are distributed under the Wald distribution. The first function `wald_lpdf()` provides the log of the density function for the Wald distribution. The density of the Wald distribution is the same as the density of the Inverse Gaussian distribution after reparametrization to parameters $\alpha$ (boundary) and $\nu$ (drift rate):


```{r}
writeLines(readLines(here::here("stan", "helpers", "wald_lpdf.stan")))
```

The second function `wald_rng()` implements the RNG function of a Wald distributed variable:

```{r}
writeLines(readLines(here::here("stan", "helpers", "wald_rng.stan")))
```

The description of both functions can be found under Wikipedia entry for the [Inverse Gaussian distribution](https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution).

## Truncated normal distribution

We implemented a truncated normal distributions in Stan. These are not, however, used in the model itself. It could be argued that using truncated normal distributions will make more sense for some of the parts of the model, as the eye-tracking data is constrained by the dimensions of the screen. However, we did not thoroughly test and use the model with truncated normal distributions, and did not use it. In general, the bias in parameters created by omitting the truncation should be relatively small, as most of the mass of the gaussian distributions in our model was concentrated far away from the truncation bounds.

However, in some of the simulations, we used truncated normal distribution to be able to generate data that are more realistic (i.e., do not go outside of the screen dimensions.)

The following function implements the density of the truncated normal distribution (with `lb` the lower bound and `ub` the upper bound). 

```{r}
writeLines(readLines(here::here("stan", "helpers", "trunc_normal_lpdf.stan")))
```

And the function below implements generating data from truncated normal distribution.

```{r}
writeLines(readLines(here::here("stan", "helpers", "trunc_normal_rng.stan")))
```

The functions should be self-explanatory, but for the sake of completeness consult the related post on a Stan forum [related post on a Stan forum](https://discourse.mc-stan.org/t/rng-for-truncated-distributions/3122/3) should there be any confusion.


## Log density of a gaussian mixture

For modelling the distribution of fixation locations, we  used a mixture of bivariate normal distributions for some of the "factors":

$$
f(x, y) = \sum_{k=1}^{K} \pi_k \times \text{Normal}(x | \mu_{kx}, \sigma_{kx}) \times \text{Normal}(y | \mu_{ky}, \sigma_{ky}),
$$

with $\mu_{kx}$ and $\sigma_{kx}$ being the center and width of a factor $k$ in the $x$ dimension, $\mu_{yx}$ and $\sigma_{yx}$ being the center and width of a factor $k$ in the $y$ dimension, and $\pi_k$ the weight of a factor $k$.

To obtain the log of this density, we constructed the following  helper functions `mixture_normals()` and `mixture_trunc_normals()` that implement this calculations. The latter function implements the truncated mixture and requires to specify the lower and upper truncation bounds on the x and y dimensions.


```{r}
writeLines(readLines(here::here("stan", "helpers", "mixture_normals_lpdf.stan")))
```

## Common functions to compute the drift rate $\nu$

The model we use relies on integration of the intensity function with the attention window. Recall that we define the intensity function as
and that we defined the attention window as a kernel of a Gaussian distribution:

$$
a(x, y | s) = \exp \left(-\frac{(x - s_x)^2}{2\sigma_a^2}\right) \times \exp \left(-\frac{(y - s_y)^2}{2\sigma_a^2}\right)
$$

The following function `log_integral_attention_1d()` implements the integral of an intensity function in one dimension multiplied by the attention window:


$$
\log \int \frac{1}{\sqrt{2\pi}\sigma_x}\exp\left(-\frac{(x - \mu_x)^2}{2\sigma_x^2}\right) \exp\left(-\frac{(x - s_x)^2}{2\sigma_a^2}\right) dx = \frac{\sigma_a}{\sqrt{\sigma_a^2+\sigma_x^2}} \exp\left[-\frac{(\mu_x-s_x)^2}{2(\sigma_a^2 + \sigma_x^2)}\right],
$$

which is the "information" available along the x dimension given the fixation at location $s_x$, the attention window width $\sigma_a$, and the location and spread of some factor $\mu_x$ and $\sigma_x$.

```{r echo=FALSE}
writeLines(readLines(here::here("stan", "helpers", "log_integral_attention_1d.stan")))
```

And the function in `log_integral_attention_mixture_2d()` implements the integration:

$$
\int\int \lambda(x, y) \times a(x, y | s) dx dy,
$$

which basically combines `log_integral_attention_1d()` for x and y dimension separately for each "factor", and weights them by their `weights`.

```{r echo=FALSE}
writeLines(readLines(here::here("stan", "helpers", "log_integral_attention_mixture_2d.stan")))
```

## Saliency


```{r}
writeLines(readLines(here::here("stan", "helpers", "saliency_rng.stan")))
```

## Direction bias

## References
